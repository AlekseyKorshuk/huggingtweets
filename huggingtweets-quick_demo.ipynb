{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingTweets - Tweet Generation with Huggingface - Quick Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: this demo is not to be used to publish any false generated information but to perform research on Natural Language Generation (NLG).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-quick_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Generating realistic text has become more and more efficient with models such as [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). Those models are trained on very large datasets and require heavy computer resources (and time!).\n",
    "\n",
    "However, we can use Transfer Learning and a single GPU to quickly fine-tune a pre-trained model on a given task.\n",
    "\n",
    "We test if we can imitate the writing style of a Twitter user by only using some of his tweets. Twitter API let us download \"only\" the 3200 most recent tweets from any single user, which we then filter out (to remove retweets, short content, etc).\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) gives us an easy access to pre-trained models and fine-tuning techniques for Natural Language Generation (NLG) tasks.\n",
    "\n",
    "We will be monitoring the training with [W&B](https://docs.wandb.com/huggingface) (which is integrated in HuggingFace) to ensure the model is learning from the data and compare multiple experiments.\n",
    "\n",
    "![](https://i.imgur.com/vnejHGh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if those libraries are not installed\n",
    "!pip install torch -qq\n",
    "!pip install git+https://github.com/huggingface/transformers.git -qq\n",
    "!pip install wandb -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface scripts for fine-tuning models and language generation\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py -q\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/text-generation/run_generation.py -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tweets from a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download latest tweets associated to a user account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- Enter the screen name of the user you will download your dataset from --->\n",
    "handle = 'l2k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib3\n",
    "http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
    "res = http.request(\"GET\", f\"https://us-central1-playground-111.cloudfunctions.net/tweets_http?handle={handle}\")\n",
    "curated_tweets = json.loads(res.data.decode('utf-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Curated tweets: {len(curated_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Curated tweets\\n')\n",
    "for t in curated_tweets[:5]:\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset from downloaded tweets\n",
    "\n",
    "We remove:\n",
    "* retweets (since it's not in the wording style of target author)\n",
    "* tweets with no interesting content (limited to url's, user mentionss, \"thank you\"â€¦)\n",
    "\n",
    "We clean up remaining tweets:\n",
    "* we remove url's\n",
    "* we replace \"@\" mentions with user names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove boring tweets (tweets with only urls or too short) and cleanup texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_tweet(tweet):\n",
    "    \"Clean tweet text\"\n",
    "    text = ' '.join(t for t in tweet.split() if 'http' not in t)\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    if text.split() and text.split()[0] == '.':\n",
    "         text = ' '.join(text.split()[1:])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boring_tweet(tweet):\n",
    "    \"Check if this is a boring tweet\"\n",
    "    boring_stuff = ['http', '@', '#', 'thank', 'thanks', 'I', 'you']\n",
    "    if len(tweet.split()) < 3:\n",
    "        return True\n",
    "    if all(any(bs in t.lower() for bs in boring_stuff) for t in tweet):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = [cleanup_tweet(t) for t in curated_tweets]\n",
    "cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
    "print(f'Curated tweets: {len(curated_tweets)}\\nCool tweets: {len(cool_tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quick demo, we just use all the data for training and export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(cool_tweets)\n",
    "\n",
    "with open('{}_train.txt'.format(handle), 'w') as f:\n",
    "    f.write('\\n'.join(cool_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log and monitor training through W&B\n",
    "\n",
    "In order to check our model is training correctly and compare experiments, we are going to use the W&B integration from huggingface.\n",
    "\n",
    "We log in anonymous mode for this quick demo. When running real experiments, it is better to use the [full notebook](https://github.com/borisdayma/huggingtweets/blob/master/huggingtweets.ipynb) and create a W&B account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(anonymous='allow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "HuggingFace includes the script `run_language_modeling` making it easy to fine-tune a pre-trained model.\n",
    "\n",
    "We use a pre-trained GPT-2 model and fine-tune it on our dataset.\n",
    "\n",
    "Training is automatically logged on W&B (see [documentation](https://docs.wandb.com/huggingface)). Urls are generated to visualize ongoing runs.\n",
    "\n",
    "![](https://i.imgur.com/1uIxLFe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate run to a project (optional)\n",
    "%env WANDB_PROJECT=huggingtweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use HuggingFace script `run_language_modeling.py` to fine-tune our model (see [doc](https://huggingface.co/transformers/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modeling.py \\\n",
    "    --output_dir=output/$handle \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=$handle\\_train.txt \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test our trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our model on a few sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES = [\"I think that\",\n",
    "            \"I like\",\n",
    "            \"I don't like\",\n",
    "            \"I want\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use HuggingFace script `run_generation.py` to generate sentences (see [doc](https://huggingface.co/transformers/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = random.randint(0, 2**32-1)\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --model_name_or_path output/$handle \\\n",
    "        --length 150 \\\n",
    "        --stop_token \"{'\\n'}\" \\\n",
    "        --num_return_sequences 3 \\\n",
    "        --temperature 1 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"' + start + '\"'}\n",
    "    generated = [val[-1-2*k] for k in range(3)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')\n",
    "        examples.append([start, g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log the results on our previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve last run\n",
    "project = %env WANDB_PROJECT\n",
    "wandb_id = wandb.api.list_runs(project)[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log results on our previous wandb run\n",
    "wandb.init(id=wandb_id, resume='must')\n",
    "wandb.log({'examples': wandb.Table(data=examples, columns=['Input', 'Prediction'])})\n",
    "\n",
    "# Update display name\n",
    "wandb.run.name = handle\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: Open your generated \"Run page\" generated and look at the predictions in the \"Media\" panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see my trained models on my dashboard.\n",
    "\n",
    "### [W&B Dashboard â†’](https://app.wandb.ai/borisd13/huggingface-twitter)\n",
    "\n",
    "Please share your experiments and any insights you have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
