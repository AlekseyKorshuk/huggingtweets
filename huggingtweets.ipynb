{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingTweets - Tweet Generation with Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: this demo is not to be used to publish any false generated information but to perform research on Natural Language Generation (NLG).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Generating realistic text has become more and more efficient with models such as [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). Those models are trained on very large datasets and require heavy computer resources (and time!).\n",
    "\n",
    "However, we can use Transfer Learning and a single GPU to quickly fine-tune a pre-trained model on a given task.\n",
    "\n",
    "We test if we can imitate the writing style of a Twitter user by only using some of his tweets. Twitter API let us download \"only\" the 3200 most recent tweets from any single user, which we then filter out (to remove retweets, short content, etc).\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) gives us an easy access to pre-trained models and fine-tuning techniques for Natural Language Generation (NLG) tasks.\n",
    "\n",
    "We will be monitoring the training with [W&B](https://docs.wandb.com/huggingface) (which is integrated in HuggingFace) to ensure the model is learning from the data and compare multiple experiments.\n",
    "\n",
    "![](https://i.imgur.com/vnejHGh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if those libraries are not installed\n",
    "!pip install torch -qq\n",
    "!pip install git+https://github.com/huggingface/transformers.git -qq\n",
    "!pip install wandb -qq\n",
    "!pip install tweepy -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface scripts for fine-tuning models and language generation\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py -q\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/text-generation/run_generation.py -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a Twitter Development Account\n",
    "\n",
    "In order to access Twitter data, we need to:\n",
    "\n",
    "* [create a Twitter development account](https://developer.twitter.com/en/apply-for-access)\n",
    "* [create a Twitter app](https://developer.twitter.com/en/apps)\n",
    "* get your consumer API keys: \"API key\" and \"API secret key\"\n",
    "\n",
    "The entire process only takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- Enter your credentials (don't share with anyone) --->\n",
    "consumer_key = 'XXXXXXXXXXXXXXXXXXXXXX'\n",
    "consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tweets from a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download latest tweets associated to a user account through [Tweepy](http://docs.tweepy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab all available tweets (limited to 3200 per API limitations) based on Twitter handle.\n",
    "\n",
    "**Note**: Protected users may only be requested when the authenticated user either \"owns\" the timeline or is an approved follower of the owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- Enter the screen name of the user you will download your dataset from --->\n",
    "handle = 'l2k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://gist.github.com/onmyeoin/62c72a7d61fc840b2689b2cf106f583c\n",
    "\n",
    "# initialize a list to hold all the tweepy Tweets & list with no retweets\n",
    "alltweets = []\n",
    "\n",
    "# make initial request for most recent tweets with extended mode enabled to get full tweets\n",
    "new_tweets = api.user_timeline(\n",
    "    screen_name=handle, tweet_mode='extended', count=200)\n",
    "\n",
    "# save most recent tweets\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "# save the id of the oldest tweet less one\n",
    "oldest = alltweets[-1].id - 1\n",
    "\n",
    "# check we cannot get more tweets\n",
    "no_tweet_count = 0\n",
    "\n",
    "# keep grabbing tweets until the api limit is reached\n",
    "while True:\n",
    "    print(f'getting tweets before id {oldest}')\n",
    "\n",
    "    # all subsequent requests use the max_id param to prevent duplicates\n",
    "    new_tweets = api.user_timeline(\n",
    "        screen_name=handle, tweet_mode='extended', count=200, max_id=oldest)\n",
    "    \n",
    "    # stop if no more tweets (try a few times as they sometimes eventually come)\n",
    "    if not new_tweets:\n",
    "        no_tweet_count +=1\n",
    "    else:\n",
    "        no_tweet_count = 0\n",
    "    if no_tweet_count > 5: break\n",
    "\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # update the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print(f'...{len(alltweets)} tweets downloaded so far')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset from downloaded tweets\n",
    "\n",
    "We remove:\n",
    "* retweets (since it's not in the wording style of target author)\n",
    "* tweets with no interesting content (limited to url's, user mentionss, \"thank you\"…)\n",
    "\n",
    "We clean up remaining tweets:\n",
    "* we remove url's\n",
    "* we replace \"@\" mentions with user names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_handle:\n",
    "    'Get a user handle and cache it to avoid calling too much twitter api.'\n",
    "    handles = {}\n",
    "    def get(handle):\n",
    "        if handle not in user_handle.handles.keys():            \n",
    "            try:\n",
    "                user_handle.handles[handle] = api.get_user(handle).name\n",
    "            except:\n",
    "                user_handle.handles[handle] = None\n",
    "        return user_handle.handles[handle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_handle(word):\n",
    "    'Replace user handles, remove \"@\" and \"#\"'\n",
    "    if word[0] == '@':\n",
    "        handle = re.search('^@(\\w)+', word)\n",
    "        if handle:\n",
    "            user = user_handle.get(handle.group())\n",
    "            if user is not None: return user + word[handle.endpos:]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_tweet(tweet):\n",
    "    'Return true if not a retweet'\n",
    "    if hasattr(tweet, 'retweeted_status'):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_tweets(tweets):\n",
    "    'Decide which tweets we keep and replace handles'\n",
    "    curated_tweets = []\n",
    "    for tweet in tweets:\n",
    "        if keep_tweet(tweet):\n",
    "            curated_tweets.append(' '.join(replace_handle(w) for w in tweet.full_text.split()))\n",
    "    return curated_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_tweets = curate_tweets(alltweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify our list of tweets is well curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of tweets: {len(alltweets)}\\nCurated tweets: {len(curated_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original tweets\\n')\n",
    "for t in alltweets[:5]:\n",
    "    print(f'{t.full_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Curated tweets\\n')\n",
    "for t in curated_tweets[:5]:\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove boring tweets (tweets with only urls or too short) and cleanup texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_tweet(tweet):\n",
    "    \"Clean tweet text\"\n",
    "    text = ' '.join(t for t in tweet.split() if 'http' not in t)\n",
    "    text = text.replace('&amp;', 'and')\n",
    "    if text.split() and text.split()[0] == '.':\n",
    "         text = ' '.join(text.split()[1:])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boring_tweet(tweet):\n",
    "    \"Check if this is a boring tweet\"\n",
    "    boring_stuff = ['http', '@', '#', 'thank', 'thanks', 'I', 'you']\n",
    "    if len(tweet.split()) < 3:\n",
    "        return True\n",
    "    if all(any(bs in t.lower() for bs in boring_stuff) for t in tweet):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = [cleanup_tweet(t) for t in curated_tweets]\n",
    "cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
    "print(f'Curated tweets: {len(curated_tweets)}\\nCool tweets: {len(cool_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER VERSION\n",
    "handle = \"vanpelt\"\n",
    "import json\n",
    "import urllib3\n",
    "http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
    "res = http.request(\"GET\", f\"https://us-central1-playground-111.cloudfunctions.net/tweets_http?handle={handle}\")\n",
    "curated_tweets = json.loads(res.data.decode('utf-8')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into training and validation sets (90/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(cool_tweets)\n",
    "\n",
    "# fraction of training data\n",
    "split_train_valid = 0.9\n",
    "\n",
    "# split dataset\n",
    "train_size = int(split_train_valid * len(cool_tweets))\n",
    "valid_size = len(cool_tweets) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(cool_tweets, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER VERSION\n",
    "with open('{}_train.txt'.format(handle), 'w') as f:\n",
    "    f.write('\\n'.join(cool_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export our datasets as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}_train.txt'.format(handle), 'w') as f:\n",
    "    f.write('\\n'.join(train_dataset))\n",
    "\n",
    "with open('{}_valid.txt'.format(handle), 'w') as f:\n",
    "    f.write('\\n'.join(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log and monitor training through W&B\n",
    "\n",
    "In order to check our model is training correctly and compare experiments, we are going to use the W&B integration from huggingface.\n",
    "\n",
    "### [Sign up for a free account →](https://app.wandb.ai/login?signup=true)\n",
    "\n",
    "- **Unified dashboard**: Central repository for all your model metrics and predictions\n",
    "- **Lightweight**: No code changes required to integrate with Hugging Face\n",
    "- **Accessible**: Free for individuals and academic teams\n",
    "- **Secure**: All projects are private by default\n",
    "- **Trusted**: Used by machine learning teams at OpenAI, Toyota, GitHub, Lyft and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key\n",
    "Once you've signed up, run the next cell and click on the link to get your API key and authenticate this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "HuggingFace includes the script `run_language_modeling` making it easy to fine-tune a pre-trained model.\n",
    "\n",
    "We use a pre-trained GPT-2 model and fine-tune it on our dataset.\n",
    "\n",
    "Training is automatically logged on W&B (see [documentation](https://docs.wandb.com/huggingface)). Urls are generated to visualize ongoing runs or you can just open your [dashboard](http://app.wandb.ai/).\n",
    "\n",
    "I quickly tested running for several epochs and my run was showing I started overfitting after 4 epochs so this is the limit I use to fine-tune my model (takes less than 2 minutes).\n",
    "\n",
    "![](https://i.imgur.com/1uIxLFe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate run to a project (optional)\n",
    "%env WANDB_PROJECT=huggingface-twitter-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use HuggingFace script `run_language_modeling.py` to fine-tune our model (see [doc](https://huggingface.co/transformers/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modeling.py \\\n",
    "    --output_dir=output/$handle \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=$handle\\_train.txt \\\n",
    "    --do_eval --eval_data_file=$handle\\_valid.txt \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test our trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our model on a few sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES = [\"I think that\",\n",
    "            \"I like\",\n",
    "            \"I don't like\",\n",
    "            \"I want\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use HuggingFace script `run_generation.py` to generate sentences (see [doc](https://huggingface.co/transformers/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = random.randint(0, 2**32-1)\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --model_name_or_path output/$handle \\\n",
    "        --length 150 \\\n",
    "        --stop_token \"{'\\n'}\" \\\n",
    "        --num_return_sequences 3 \\\n",
    "        --temperature 1 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"' + start + '\"'}\n",
    "    generated = [val[-1-2*k] for k in range(3)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')\n",
    "        examples.append([start, g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log the results on our previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve last run\n",
    "project = %env WANDB_PROJECT\n",
    "wandb_id = wandb.api.list_runs(project)[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log results on our previous wandb run\n",
    "wandb.init(id=wandb_id, resume='must')\n",
    "wandb.log({'examples': wandb.Table(data=examples, columns=['Input', 'Prediction'])})\n",
    "\n",
    "# Update display name\n",
    "wandb.run.name = alltweets[0].author.name\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: Open your generated \"Run page\" generated and look at the predictions in the \"Media\" panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see my trained models on my dashboard.\n",
    "\n",
    "### [W&B Dashboard →](https://app.wandb.ai/borisd13/huggingface-twitter)\n",
    "\n",
    "Please share your experiments and any insights you have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
