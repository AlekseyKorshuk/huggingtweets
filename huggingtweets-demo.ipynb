{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "huggingtweets-demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHAWMW78AG9",
        "colab_type": "text"
      },
      "source": [
        "# HuggingTweets - Train a model to generate tweets\n",
        "\n",
        "Choose your favorite Twitter account and train a language model to write new tweets based on their unique voice in just 5 minutes.\n",
        "\n",
        "Here is an example where I fine-tuned a neural network to predict Elon Musk's next breakthrough ðŸ˜‰\n",
        "\n",
        "![huggingtweets illustration](https://raw.githubusercontent.com/borisdayma/huggingtweets/master/img/example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwj6GL6LJvDI",
        "colab_type": "text"
      },
      "source": [
        "## To start the demo, click on menu at top, \"Runtime\" â†’ \"Run all\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ZSCf6QyF8AG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "\n",
        "def print_html(x):\n",
        "    \"Better printing\"\n",
        "    x = x.replace('\\n', '<br>')\n",
        "    display(HTML(x))\n",
        "        \n",
        "# Check we use GPU\n",
        "import torch\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "if not torch.cuda.is_available():\n",
        "    print_html('Error: GPU was not found\\n1/ click on the \"Runtime\" menu and \"Change runtime type\"\\n'\\\n",
        "          '2/ set \"Hardware accelerator\" to \"GPU\" and click \"save\"\\n3/ click on the \"Runtime\" menu, then \"Run all\" (below error should disappear)')\n",
        "    raise ValueError('No GPU available')\n",
        "else:\n",
        "    # Install dependencies\n",
        "    !pip install wandb transformers torch -qq\n",
        "\n",
        "    import ipywidgets as widgets\n",
        "    from IPython import get_ipython\n",
        "    import json\n",
        "    import urllib3\n",
        "    import pathlib\n",
        "    import shutil\n",
        "    import requests\n",
        "    import wandb\n",
        "    wandb.login(anonymous='allow')  # ensure we log with huggingface\n",
        "    from transformers import (\n",
        "        AutoTokenizer, AutoModelForCausalLM,\n",
        "        TextDataset, DataCollatorForLanguageModeling,\n",
        "        Trainer, TrainingArguments)\n",
        "    from transformers.hf_api import HfApi\n",
        "    \n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "    \n",
        "    log_debug = widgets.Output()\n",
        "\n",
        "    \n",
        "    # Have global access to model, tokenizer & dataset\n",
        "    trainer, tokenizer = None, None\n",
        "    artifact_dataset = None\n",
        "    card_val = {}\n",
        "\n",
        "    # Associate run to a project\n",
        "    with log_debug:\n",
        "        %env WANDB_PROJECT=huggingtweets\n",
        "        %env WANDB_WATCH=false\n",
        "        %env WANDB_ENTITY=borisd13\n",
        "        %env WANDB_ANONYMOUS=allow\n",
        "        %env WANDB_NOTEBOOK_NAME=huggingtweets-demo\n",
        "        %env WANDB_NOTES=Github repo: https://github.com/borisdayma/huggingtweets\n",
        "        HW_VERSION = 0.3\n",
        "\n",
        "    def fix_text(text):\n",
        "        text = text.replace('&amp;', '&')\n",
        "        text = text.replace('&lt;', '<')\n",
        "        text = text.replace('&gt;', '>')\n",
        "        return text\n",
        "\n",
        "    def html_table(data, title=None):\n",
        "        'Create a html table'\n",
        "        width_twitter = '75px'\n",
        "        def html_cell(i, twitter_button=False):\n",
        "            return f'<td style=\"width:{width_twitter}\">{i}</td>' if twitter_button else f'<td>{i}</td>'\n",
        "        def html_row(row):\n",
        "            return f'<tr>{\"\".join(html_cell(r, not i if len(row)>1 else False) for i,r in enumerate(row))}</tr>'    \n",
        "        body = f'<table style=\"width:100%\">{\"\".join(html_row(r) for r in data)}</table>'\n",
        "        title_html = f'<h3>{title}</h3>' if title else ''\n",
        "        html = '''\n",
        "        <html>\n",
        "            <head>\n",
        "                <style>\n",
        "                    table {border-collapse: collapse !important;}\n",
        "                    td {text-align:left !important; border: solid #E3F2FD !important; border-width: 1px 0 !important; padding: 6px !important;}\n",
        "                    tr:nth-child(even) {background-color: #E3F2FD !important;}\n",
        "                    tr:nth-child(odd) {background-color: #FFFFFF !important;}\n",
        "                </style>\n",
        "            </head>\n",
        "            <body>''' + title_html + body + '</body></html>'\n",
        "        return(html)\n",
        "\n",
        "    def clean_tweet(tweet):\n",
        "        bad_start = ['http', 'pic']\n",
        "        text = ' '.join(t for t in tweet.split() if not(any(bs in t for bs in bad_start)))\n",
        "        return text\n",
        "        \n",
        "    def boring_tweet(tweet):\n",
        "        \"Check if this is a boring tweet\"\n",
        "        boring_stuff = ['http', '@', '#', 'thank', 'thanks', 'I', 'you']\n",
        "        if len(tweet.split()) < 3:\n",
        "            return True\n",
        "        if all(any(bs in t.lower() for bs in boring_stuff) for t in tweet.split()):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def create_model_card(card_val, output_dir):\n",
        "        model_card_url = 'https://github.com/borisdayma/huggingtweets/raw/master/model_card/README.md'\n",
        "        model_card = requests.get(model_card_url).content.decode('utf-8')\n",
        "        for k, v in card_val.items():\n",
        "            model_card = model_card.replace(k, v)\n",
        "        with open(f'{output_dir}/README.md', 'w') as f:\n",
        "            f.write(model_card)\n",
        "\n",
        "    def dl_tweets():\n",
        "        with log_debug:\n",
        "            wandb.join()  # ensure any previous run is closed\n",
        "        handle_widget.disabled = True\n",
        "        run_dl_tweets.disabled = True\n",
        "        run_dl_tweets.button_style = 'primary'\n",
        "\n",
        "        get_ipython().kernel.do_one_iteration() # widget slow to update on phones\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle = handle.lower().strip()\n",
        "        log_dl_tweets.clear_output(wait=True)\n",
        "\n",
        "        success_try = False\n",
        "\n",
        "        with log_dl_tweets:\n",
        "            try:\n",
        "                print_html(f'\\nDownloading {handle_widget.value.strip()} tweets... This should take no more than a minute!')\n",
        "                http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
        "                res = http.request(\"GET\", f\"http://us-central1-huggingtweets.cloudfunctions.net/get_tweets?handle={handle}&force=1\")\n",
        "                res = json.loads(res.data.decode('utf-8'))\n",
        "                \n",
        "                # save user info\n",
        "                card_val['USER_HANDLE'] = handle\n",
        "                card_val['USER_NAME'] = res['user_name']\n",
        "                card_val['USER_PROFILE'] = res['user_profile'].replace('_normal.', '_400x400.')\n",
        "\n",
        "                all_tweets = res['tweets']\n",
        "                curated_tweets = [fix_text(tweet) for tweet in all_tweets]\n",
        "                log_dl_tweets.clear_output(wait=True)\n",
        "                print_html(f\"\\n{res['n_tweets']} tweets from {handle_widget.value.strip()} downloaded!\\n\\n\")\n",
        "                    \n",
        "                # create dataset\n",
        "                clean_tweets = [clean_tweet(tweet) for tweet in curated_tweets]\n",
        "                cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
        "                total_text = '<|endoftext|> ' + ' <|endoftext|> '.join(cool_tweets) + ' <|endoftext|>'\n",
        "\n",
        "                # display a few tweets\n",
        "                display(HTML(html_table([[t] for t in curated_tweets[:8]])))\n",
        "                get_ipython().kernel.do_one_iteration() # time for auto-scroll\n",
        "                \n",
        "                if len(total_text) < 6000:\n",
        "                    # need about 4000 chars for one data sample (but depends on spaces, etc)\n",
        "                    raise ValueError(f\"Error: this user does not have enough tweets to train a Neural Network\\n{res['n_tweets']} tweets downloaded, including {res['n_RT']} RT's and {len(all_tweets) - len(cool_tweets)} boring tweets... only {len(cool_tweets)} tweets kept!\")\n",
        "                if len(total_text) < 40000:\n",
        "                    print_html('\\n<b>Warning: this user does not have many tweets which may impact the results of the Neural Network</b>')\n",
        "                    \n",
        "                print_html('\\nCreating dataset...')\n",
        "                get_ipython().kernel.do_one_iteration() # time for auto-scroll\n",
        "                    \n",
        "                # log dataset\n",
        "                with log_debug:\n",
        "                    with wandb.init(name=f'@{handle}-dl_data', job_type='dl_data', config={'version':HW_VERSION}, reinit=True) as run:\n",
        "                        # log raw tweets as input\n",
        "                        artifact_input = wandb.Artifact(\n",
        "                            f'tweets-{handle}',\n",
        "                            type='raw-dataset',\n",
        "                            description=f'Raw tweets from @{handle} downloaded with Tweepy',\n",
        "                            metadata={'handle':handle,\n",
        "                                      'tweets downloaded': res['n_tweets'],\n",
        "                                      'retweets': res['n_RT'],\n",
        "                                      'tweets kept': len(all_tweets),\n",
        "                                      'huggingtweets version': HW_VERSION})\n",
        "                        with artifact_input.new_file('tweets.txt') as f:\n",
        "                            json.dump(all_tweets, f, indent=0)\n",
        "                        run.log_artifact(artifact_input)\n",
        "\n",
        "                    with wandb.init(name=f'@{handle}-preprocess', job_type='preprocess', config={'version':HW_VERSION}, reinit=True) as run:\n",
        "                        run.use_artifact(artifact_input)\n",
        "                        # log dataset as output\n",
        "                        global artifact_dataset\n",
        "                        artifact_dataset = wandb.Artifact(\n",
        "                            f'dataset-{handle}',\n",
        "                            type='train-dataset',\n",
        "                            description=f'Dataset created from tweets of @{handle}',\n",
        "                            metadata={'handle':handle,\n",
        "                                      'tweets downloaded': res['n_tweets'],\n",
        "                                      'retweets': res['n_RT'],\n",
        "                                      'short tweets': len(all_tweets) - len(cool_tweets),\n",
        "                                      'tweets kept': len(cool_tweets),\n",
        "                                      'huggingtweets version': HW_VERSION})\n",
        "                        with open(f'data_{handle}_train.txt', 'w') as f:\n",
        "                            f.write(total_text)\n",
        "                        artifact_dataset.add_file(f'data_{handle}_train.txt')\n",
        "                        run.log_artifact(artifact_dataset)\n",
        "                        \n",
        "                        # keep track of url\n",
        "                        wandb_url = wandb.run.get_url()\n",
        "                        # remove api token\n",
        "                        i = wandb_url.find('?apiKey=')\n",
        "                        wandb_url = wandb_url[:i if i != -1 else None]\n",
        "                        card_val['WANDB_PREPROCESS'] = wandb_url\n",
        "\n",
        "                    # Save data info\n",
        "                    card_val['TWEETS_DL'] = str(res['n_tweets'])\n",
        "                    card_val['RETWEETS'] = str(res['n_RT'])\n",
        "                    card_val['SHORT_TWEETS'] = str(len(all_tweets) - len(cool_tweets))\n",
        "                    card_val['TWEETS_KEPT'] = str(len(cool_tweets))\n",
        "                \n",
        "                success_try = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_dl_tweets.button_style = 'danger'\n",
        "        \n",
        "        if success_try:\n",
        "            run_dl_tweets.button_style = 'success'\n",
        "            log_finetune.clear_output(wait=True)\n",
        "            with log_finetune:\n",
        "                print_html('\\nFine-tune your model by clicking on \"Train Neural Network\"')\n",
        "            run_finetune.disabled = False\n",
        "            print_html(f\"\\nðŸŽ‰ Dataset created: {res['n_tweets']} tweets downloaded, including {res['n_RT']} RT's and {len(all_tweets) - len(cool_tweets)} short tweets... keeping {len(cool_tweets)} tweets\")\n",
        "\n",
        "        handle_widget.disabled = False\n",
        "        run_dl_tweets.disabled = False\n",
        "                \n",
        "    handle_widget = widgets.Text(value='@elonmusk',\n",
        "                                placeholder='Enter twitter handle')\n",
        "\n",
        "    run_dl_tweets = widgets.Button(\n",
        "        description='Download tweets',\n",
        "        button_style='primary')\n",
        "    def on_run_dl_tweets_clicked(b):\n",
        "        dl_tweets()\n",
        "    run_dl_tweets.on_click(on_run_dl_tweets_clicked)\n",
        "\n",
        "    log_restart = widgets.Output()\n",
        "    log_dl_tweets = widgets.Output()\n",
        "    \n",
        "    def finetune():\n",
        "        if run_finetune.button_style == 'success':\n",
        "            # user double clicked before start of function\n",
        "            return\n",
        "\n",
        "        handle_widget.disabled = True\n",
        "        run_dl_tweets.disabled = True\n",
        "        run_finetune.disabled = True\n",
        "        run_finetune.button_style = 'primary'\n",
        "\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle = handle.lower()\n",
        "        log_finetune.clear_output(wait=True)\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        success_try = False\n",
        "\n",
        "        with log_finetune:\n",
        "            print_html(f'\\nTraining Neural Network on {handle_widget.value.strip()} tweets... This could take up to 2-3 minutes!\\n')\n",
        "            progress = widgets.FloatProgress(value=0.1, min=0.0, max=1.0, bar_style = 'info')\n",
        "            display(progress)\n",
        "\n",
        "        with log_debug:\n",
        "            try:                \n",
        "                # Setting up pre-trained neural network\n",
        "                with log_finetune:\n",
        "                    print_html('\\nSetting up pre-trained neural network...')\n",
        "                global trainer, tokenizer\n",
        "                tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "                model = AutoModelForCausalLM.from_pretrained('gpt2', cache_dir=pathlib.Path('cache').resolve())\n",
        "                block_size = tokenizer.max_len\n",
        "                train_dataset = TextDataset(tokenizer=tokenizer, file_path=f'data_{handle}_train.txt', block_size=block_size, overwrite_cache=True)\n",
        "                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "                epochs = 4  # limit before overfitting\n",
        "                seed = 123\n",
        "                training_args = TrainingArguments(\n",
        "                    output_dir=f'output/{handle}',\n",
        "                    overwrite_output_dir=True,\n",
        "                    do_train=True,\n",
        "                    num_train_epochs=epochs,\n",
        "                    per_device_train_batch_size=1,\n",
        "                    logging_steps=5,\n",
        "                    save_steps=0,\n",
        "                    seed=seed)\n",
        "                \n",
        "                # create wandb run (before it's done automatically by Trainer)\n",
        "                run = wandb.init(name=f'@{handle}-train', job_type='train', config={'huggingtweets version':HW_VERSION, **vars(training_args)}, reinit=True)\n",
        "                \n",
        "                # keep track of url\n",
        "                wandb_url = wandb.run.get_url()\n",
        "                # remove api token\n",
        "                i = wandb_url.find('?apiKey=')\n",
        "                wandb_url = wandb_url[:i if i != -1 else None]\n",
        "                card_val['WANDB_TRAIN'] = wandb_url\n",
        "\n",
        "                trainer = Trainer(\n",
        "                    model=model,\n",
        "                    args=training_args,\n",
        "                    data_collator=data_collator,\n",
        "                    train_dataset=train_dataset,\n",
        "                    prediction_loss_only=True)\n",
        "                progress.value = 0.3\n",
        "\n",
        "                # log dataset and pretrained model\n",
        "                run.use_artifact(artifact_dataset)\n",
        "                artifact_gpt2 = wandb.Artifact(\n",
        "                    f'gpt2',\n",
        "                    type='pretrained-model',\n",
        "                    description=f'Pretrained model from OpenAI downloaded from ðŸ¤— Transformers: https://huggingface.co/gpt2',\n",
        "                    metadata={'huggingtweets version': HW_VERSION})\n",
        "                artifact_gpt2.add_dir('cache', name='gpt2')\n",
        "                run.use_artifact(artifact_gpt2)\n",
        "                progress.value = 0.4\n",
        "                \n",
        "                p_start, p_end = 0.4, 0.8\n",
        "                def progressify(f):\n",
        "                    \"Control progress bar when calling f\"\n",
        "                    def inner(*args, **kwargs):\n",
        "                        if trainer.epoch is not None:\n",
        "                            progress.value = p_start + trainer.epoch / epochs * (p_end - p_start)\n",
        "                        return f(*args, **kwargs)\n",
        "                    return inner\n",
        "        \n",
        "                trainer._training_step = progressify(trainer._training_step)\n",
        "                \n",
        "                # Training neural network\n",
        "                with log_finetune:\n",
        "                    print_html('Training neural network...\\n')\n",
        "                    display(wandb.jupyter.Run())\n",
        "                    print_html('\\n')\n",
        "                    display(progress)\n",
        "                trainer.train()\n",
        "\n",
        "                # set model config parameters\n",
        "                trainer.model.config.task_specific_params['text-generation'] = {\n",
        "                    'do_sample': True,\n",
        "                    'min_length': 10,\n",
        "                    'max_length': 160,\n",
        "                    'temperature': 1.,\n",
        "                    'top_p': 0.95,\n",
        "                    'padding_text': '<|endoftext|> '}\n",
        "\n",
        "                # log fine-tuned model\n",
        "                artifact_trained = wandb.Artifact(\n",
        "                    f'model-{handle}',\n",
        "                    type='finetuned-model',\n",
        "                    description=f'Model fine-tuned on tweets from @{handle}',\n",
        "                    metadata={'handle':handle, 'seed':seed, 'huggingtweets version': HW_VERSION})\n",
        "                trainer.save_model(f'model-{handle}')\n",
        "                artifact_trained.add_dir(f'model-{handle}', name='model')\n",
        "                run.log_artifact(artifact_trained)\n",
        "                progress.value = 0.85\n",
        "                \n",
        "                try:\n",
        "                    # Get token\n",
        "                    hfapi = HfApi()\n",
        "                    user, namespace = 'huggingtweets-app', 'huggingtweets'\n",
        "                    token = hfapi.login(user, namespace)\n",
        "                    assert hfapi.whoami(token)[0] == user, \"Could not log into Hugging Face\"\n",
        "\n",
        "                    # create a folder with model files\n",
        "                    model_name = handle\n",
        "                    shutil.rmtree(model_name, ignore_errors=True)\n",
        "                    trainer.save_model(model_name)\n",
        "                    tokenizer.save_pretrained(model_name)\n",
        "                    valid_files = ['config.json',\n",
        "                                   'pytorch_model.bin',\n",
        "                                   'special_tokens_map.json',\n",
        "                                   'tokenizer_config.json',\n",
        "                                   'vocab.json',\n",
        "                                   'merges.txt',\n",
        "                                   'added_tokens.json']\n",
        "                    for f in pathlib.Path(model_name).glob('*'):\n",
        "                        if f.name not in valid_files:\n",
        "                            f.unlink()\n",
        "                    create_model_card(card_val, model_name)\n",
        "\n",
        "                    # upload files\n",
        "                    model_path = pathlib.Path(model_name)\n",
        "                    assert model_path.is_dir(), f\"Expected {model_path} to be a directory\"\n",
        "                    files = [(str(f.resolve()), str(f)) for f in model_path.glob('*')]\n",
        "                    assert len(files) == 7, \"Unexpected number of files in model directory: {len(files)}\"\n",
        "                    for filepath, filename in files:\n",
        "                        access_url = hfapi.presign_and_upload(token=token, filename=filename, filepath=filepath, organization=namespace)\n",
        "                        progress.value += 0.02\n",
        "\n",
        "                except Exception as e:\n",
        "                    with log_finetune:\n",
        "                        print_html(f'\\n<b>Could not upload the model to Hugging Face</b>\\n{e}')\n",
        "\n",
        "                run_finetune.button_style = 'success'\n",
        "                run_predictions.disabled = False\n",
        "\n",
        "                progress.value = 1.0\n",
        "                progress.bar_style = 'success'\n",
        "                success_try = True\n",
        "\n",
        "                with log_finetune:\n",
        "                    print_html('\\nðŸŽ‰ Neural network trained successfully!')\n",
        "                log_predictions.clear_output(wait=True)\n",
        "                with log_predictions:\n",
        "                    print_html('\\nEnter the start of a sentence and click \"Run predictions\"')\n",
        "                with log_restart:\n",
        "                    print_html('\\n<b>To change user, refresh the page</b>\\n')\n",
        "\n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_finetune.button_style = 'danger'\n",
        "                run_finetune.disabled = False\n",
        "                            \n",
        "        if not success_try:\n",
        "            display(log_debug)\n",
        "            progress.bar_style = 'danger'\n",
        "        \n",
        "    run_finetune = widgets.Button(\n",
        "        description='Train Neural Network',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_finetune_clicked(b):\n",
        "        finetune()\n",
        "    run_finetune.on_click(on_run_finetune_clicked)\n",
        "\n",
        "    log_finetune = widgets.Output()\n",
        "    with log_finetune:\n",
        "        print_html('\\nWaiting for Step 1 to complete...')\n",
        "\n",
        "    def clean_prediction(text):\n",
        "        token = '<|endoftext|>'\n",
        "        text = text.replace(token, '')\n",
        "        text = text.strip()\n",
        "        if text[-1] == '\"' and text.count('\"') % 2: text = text[:-1]\n",
        "        return text.strip()\n",
        "\n",
        "    predictions = []\n",
        "    \n",
        "    def shorten_text(text, max_char):\n",
        "        while len(text) > max_char:\n",
        "            text = ' '.join(text.split()[:-1]) + 'â€¦'\n",
        "        return text\n",
        "        \n",
        "    def predict():\n",
        "        run_predictions.disabled = True\n",
        "        start_widget.disabled = True\n",
        "        run_predictions.button_style = 'primary'\n",
        "        handle = handle_widget.value.strip()\n",
        "        handle = handle[1:] if handle[0] == '@' else handle\n",
        "        handle_uncased = handle.strip()\n",
        "        handle = handle.lower().strip()\n",
        "        model_url = f'https://huggingface.co/huggingtweets/{handle}'\n",
        "        log_predictions.clear_output(wait=True)\n",
        "\n",
        "        # tweet buttons don't appear well in colab if within log_predictions widget\n",
        "        # we reset the entire cell\n",
        "        clear_output(wait=True)\n",
        "        display(widgets.VBox([start_widget, run_predictions, log_predictions]))\n",
        "\n",
        "        def tweet_html(tweet_text):\n",
        "            tweet_text = shorten_text(tweet_text, 238)\n",
        "            tweet_text = tweet_text.replace('\"', '&quot;')\n",
        "\n",
        "            return '<a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" data-size=\"large\" '\\\n",
        "                    f'data-text=\"{tweet_text}\" '\\\n",
        "                    f'data-url=\"{model_url}\" data-related=\"borisdayma,weights_biases,huggingface\"'\\\n",
        "                    'data-show-count=\"false\">Tweet</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>'\n",
        "\n",
        "        success_try = False\n",
        "\n",
        "        # get start sentence\n",
        "        get_ipython().kernel.do_one_iteration() # widget slow to update on phones\n",
        "        start = start_widget.value.strip()\n",
        "                \n",
        "        with log_predictions:\n",
        "            print_html(f'\\nPerforming predictions of @{handle} starting with \"{start}\"...\\nThis should take no more than 10 seconds!')\n",
        "        \n",
        "        with log_debug:\n",
        "            try:\n",
        "                # start a wandb run (should never happen)\n",
        "                if wandb.run is None:\n",
        "                    wandb.init()\n",
        "\n",
        "                wandb_url = wandb.run.get_url()\n",
        "                # remove api token\n",
        "                i = wandb_url.find('?apiKey=')\n",
        "                wandb_url = wandb_url[:i if i != -1 else None]\n",
        "                \n",
        "                # prepare input\n",
        "                start_with_bos = '<|endoftext|> ' + start\n",
        "                encoded_prompt = tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "                encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
        "\n",
        "                # prediction\n",
        "                output_sequences = trainer.model.generate(\n",
        "                    input_ids=encoded_prompt,\n",
        "                    max_length=160,\n",
        "                    min_length=10,\n",
        "                    temperature=1.,\n",
        "                    top_p=0.95,\n",
        "                    do_sample=True,\n",
        "                    num_return_sequences=8\n",
        "                    )\n",
        "                stop_token = '<|endoftext|>'\n",
        "                generated_sequences = []\n",
        "\n",
        "                # decode prediction\n",
        "                for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "                    generated_sequence = generated_sequence.tolist()\n",
        "                    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "                    limit = text.find(stop_token, 1)\n",
        "                    text = text[: limit if limit != -1 else None]\n",
        "                    # we also remove new lines which happen due to pre-trained gpt-2 and twitter scraping\n",
        "                    limit = text.find('\\n')\n",
        "                    text = text[: limit if limit != -1 else None]\n",
        "                    generated_sequence = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "                    generated_sequences.append(clean_prediction(generated_sequence))\n",
        "                \n",
        "                for i, g in enumerate(generated_sequences):\n",
        "                    predictions.append([start, ' '.join([start, g])])\n",
        "                \n",
        "                # log predictions\n",
        "                wandb.log({'examples': wandb.Table(data=predictions, columns=['Input', 'Prediction'])})\n",
        "\n",
        "                # make html table\n",
        "                tweet_data = [[tweet_html(f'Trained a neural network with huggingtweets on @{handle_uncased}: {start} â†’ {g}'), ' '.join([start, g])] for g in generated_sequences]\n",
        "                tweet_table = HTML(html_table(tweet_data))\n",
        "\n",
        "                # make model share table\n",
        "                tweet_share = f'I created an AI bot of @{handle_uncased} with huggingtweets!\\nPlay with my model or create your own!\\n\\nMade by @borisdayma using @huggingface and @weights_biases'\n",
        "                link_model = f'<a href=\"{model_url}\" rel=\"noopener\" target=\"_blank\">{model_url}</a>'\n",
        "                share_data = [[tweet_html(tweet_share),\n",
        "                               f'ðŸŽ‰ Share @{handle_uncased} model: {link_model} <i>(may take 30 seconds to become active)</i>']]\n",
        "                share_table = HTML(html_table(share_data))\n",
        "\n",
        "                run_predictions.button_style = 'success'\n",
        "                success_try = True\n",
        "                \n",
        "            except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                run_predictions.button_style = 'danger'\n",
        "\n",
        "        if success_try:\n",
        "            with log_predictions:\n",
        "                log_predictions.clear_output(wait=True)\n",
        "                \n",
        "                # somehow display works one way with Jupyter and one way with colab\n",
        "                if not IN_COLAB:\n",
        "                    print_html('\\n')\n",
        "                    display(share_table)\n",
        "                    print_html('\\n<b>Click to share your model or favorite tweets or try new predictions!</b>\\n\\n')\n",
        "                    display(tweet_table)\n",
        "                    print_html('\\n<b>Click to share your model or favorite tweets or try new predictions!</b>\\n\\n')\n",
        "            if IN_COLAB:\n",
        "                print_html('\\n')\n",
        "                display(share_table)\n",
        "                print_html('\\n<b>Click to share your model or favorite tweets or try new predictions!</b>\\n\\n')\n",
        "                display(tweet_table)\n",
        "                print_html('\\n<b>Click to share your model or favorite tweets or try new predictions!</b>\\n\\n')\n",
        "        else:\n",
        "            display(log_debug)\n",
        "        \n",
        "        run_predictions.disabled = False\n",
        "        start_widget.disabled = False\n",
        "                \n",
        "    start_widget = widgets.Text(value='My dream is',\n",
        "                                placeholder='Start a sentence')\n",
        "\n",
        "    run_predictions = widgets.Button(\n",
        "        description='Run predictions',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_predictions_clicked(b):\n",
        "        predict()\n",
        "    run_predictions.on_click(on_run_predictions_clicked)\n",
        "\n",
        "    log_predictions = widgets.Output()\n",
        "    with log_predictions:\n",
        "        print_html('\\nWaiting for Step 2 to complete...')\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print_html(\"ðŸŽ‰ Environment set-up correctly! You're ready to move to Step 1!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMpSPr0T8AHD",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 - Enter a Twitter handle\n",
        "\n",
        "Enter a Twitter handle and click Download tweets. This gives the model a dataset of examples to train on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "6O-8Kr_m8AHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "display(widgets.VBox([handle_widget, run_dl_tweets, log_restart, log_dl_tweets]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_ArgCZ8AHH",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 - Train your Neural Network\n",
        "\n",
        "Fine-tune a language model on your unique set of tweets to generate predictions. The model is downloaded from [HuggingFace Transformers](https://huggingface.co/), an awesome open source library for Natural Language Processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "CpxBQYF88AHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "display(widgets.VBox([run_finetune, log_finetune]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfRPh4V18AHM",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Generate tweets\n",
        "\n",
        "Type the beginning of a tweet, press Run predictions, and the model will try to come up with a realistic ending to your tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "FlMACi0-8AHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title â € {display-mode: \"form\"}\n",
        "if IN_COLAB:\n",
        "    display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 2000})'''))\n",
        "display(widgets.VBox([start_widget, run_predictions, log_predictions]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlRI2hsBKtz6",
        "colab_type": "text"
      },
      "source": [
        "Huggingtweets is still in its infancy and will get better over time!\n",
        "\n",
        "In the future, it will train continuously to become a Twitter expert!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4iawnVxrItM",
        "colab_type": "text"
      },
      "source": [
        "## About\n",
        "\n",
        "*Built by Boris Dayma*\n",
        "\n",
        "[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/borisdayma)\n",
        "\n",
        "My main goals with this project are:\n",
        "* to experiment with how to train, deploy and maintain neural networks in production ;\n",
        "* to make AI accessible to everyone ;\n",
        "* to have fun!\n",
        "\n",
        "For more details, visit the project repository.\n",
        "\n",
        "[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n",
        "\n",
        "**Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation.**\n",
        "\n",
        "## Resources\n",
        "\n",
        "* [Explore the W&B report](https://bit.ly/2TGXMZf) to understand how the model works\n",
        "* [HuggingFace and W&B integration documentation](https://docs.wandb.com/library/integrations/huggingface)\n",
        "* [A step by step guide](https://app.wandb.ai/jxmorris12/huggingface-demo/reports/A-Step-by-Step-Guide-to-Tracking-Hugging-Face-Model-Performance--VmlldzoxMDE2MTU): track your Hugging Face model performance\n",
        "\n",
        "## Got questions about W&B?\n",
        "\n",
        "If you have any questions about using W&B to track your model performance and predictions, please reach out to the [slack community](http://bit.ly/wandb-forum)."
      ]
    }
  ]
}