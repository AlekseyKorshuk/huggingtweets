{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "huggingtweets-demo.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHAWMW78AG9",
        "colab_type": "text"
      },
      "source": [
        "# HuggingTweets - Tweet Generation with Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGss-MZEEh4U",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8wCGVWI98gt",
        "colab_type": "text"
      },
      "source": [
        "Train in 5m your own Neural Network on someone's tweets and tweet them back your awesome predictions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwj6GL6LJvDI",
        "colab_type": "text"
      },
      "source": [
        "## To start the demo, click on \"Runtime\" menu â†’ \"Run all\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ZSCf6QyF8AG-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title â €\n",
        "\n",
        "# Check we use GPU\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    print('Error: GPU was not found\\n1/ click on the \"Runtime\" menu and \"Change runtime type\"\\n'\\\n",
        "          '2/ set \"Hardware accelerator\" to \"GPU\" and click \"save\"\\n3/ click on the \"Runtime\" menu, then \"Run all\" (below error should disappear)')\n",
        "    raise\n",
        "else:    \n",
        "    # Install dependencies\n",
        "    !pip install wandb transformers torch -qq\n",
        "\n",
        "    # Huggingface scripts for fine-tuning models and language generation\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py -q\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/text-generation/run_generation.py -q\n",
        "        \n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import json\n",
        "    import urllib3\n",
        "    import random\n",
        "    import wandb\n",
        "    import click\n",
        "\n",
        "    def fix_text(text):\n",
        "        text = text.replace('&amp;', '&')\n",
        "        text = text.replace('&lt;', '<')\n",
        "        text = text.replace('&gt;', '>')\n",
        "        return text\n",
        "\n",
        "    def cleanup_tweet(tweet):\n",
        "        \"Clean tweet text\"\n",
        "        text = ' '.join(t for t in tweet.split() if 'http' not in t)\n",
        "        if text.split() and text.split()[0] == '.':\n",
        "            text = ' '.join(text.split()[1:])\n",
        "        return text\n",
        "\n",
        "    def boring_tweet(tweet):\n",
        "        \"Check if this is a boring tweet\"\n",
        "        boring_stuff = ['http', '@', '#', 'thank', 'thanks', 'I', 'you']\n",
        "        if len(tweet.split()) < 3:\n",
        "            return True\n",
        "        if all(any(bs in t.lower() for bs in boring_stuff) for t in tweet):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def html_table(data):\n",
        "        'Create a html table'\n",
        "        def html_cell(i):\n",
        "            return f'<td>{i}</td>'\n",
        "        def html_row(r):\n",
        "            return f'<tr>{\"\".join(html_cell(i) for i in r)}</tr>'\n",
        "        return f'<table>{\"\".join(html_row(r) for r in data)}</table>'\n",
        "\n",
        "    def dl_tweets(handle_value):\n",
        "        handle = handle_value[1:] if handle_value[0] == '@' else handle_value\n",
        "        run_dl_tweets.button_style = 'primary'\n",
        "        log_dl_tweets.clear_output(wait=True)\n",
        "        with log_dl_tweets:\n",
        "            try:\n",
        "                print(f'\\nDownloading {handle_value} tweets... This should take no more than a minute!')\n",
        "                http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
        "                res = http.request(\"GET\", f\"http://us-central1-playground-111.cloudfunctions.net/tweets_http?handle={handle}\")\n",
        "                curated_tweets = json.loads(res.data.decode('utf-8'))\n",
        "                curated_tweets = [fix_text(tweet) for tweet in curated_tweets]\n",
        "                log_dl_tweets.clear_output(wait=True)\n",
        "                print(f'\\n{len(curated_tweets)} tweets from {handle_value} downloaded!')\n",
        "                random.shuffle(curated_tweets)\n",
        "                for i,t in enumerate(curated_tweets[:5]):\n",
        "                    print(f'\\nExample #{i+1}\\n{t}')\n",
        "                    \n",
        "                # create dataset\n",
        "                clean_tweets = [cleanup_tweet(t) for t in curated_tweets]\n",
        "                cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
        "                with open('{}_train.txt'.format(handle), 'w') as f:\n",
        "                    f.write('\\n'.join(cool_tweets))\n",
        "                \n",
        "                run_dl_tweets.button_style = 'success'\n",
        "                run_finetune.disabled = False\n",
        "            except Exception as e:\n",
        "                print('An error occured...\\n')\n",
        "                print(e)\n",
        "                run_dl_tweets.button_style = 'danger'\n",
        "                \n",
        "    handle_widget = widgets.Text(value='@elonmusk',\n",
        "                                placeholder='Enter twitter handle')\n",
        "\n",
        "    run_dl_tweets = widgets.Button(\n",
        "        description='Download tweets',\n",
        "        button_style='primary')\n",
        "    def on_run_dl_tweets_clicked(b):\n",
        "        dl_tweets(handle_widget.value)\n",
        "    run_dl_tweets.on_click(on_run_dl_tweets_clicked)\n",
        "\n",
        "    log_dl_tweets = widgets.Output()\n",
        "    with log_dl_tweets:\n",
        "        print('\\nEnter a Twitter handle and click \"Download tweets\"')\n",
        "        \n",
        "    # Associate run to a project\n",
        "    %env WANDB_PROJECT=huggingtweets\n",
        "    %env WANDB_WATCH=false\n",
        "    %env WANDB_ENTITY=wandb\n",
        "    wandb.login(anonymous='allow')\n",
        "\n",
        "    def finetune():\n",
        "        handle = handle_widget.value[1:] if handle_widget.value[0] == '@' else handle_widget.value\n",
        "        run_finetune.button_style = 'primary'\n",
        "        log_finetune.clear_output(wait=True)\n",
        "\n",
        "        with log_finetune:\n",
        "            try:\n",
        "                print(f'\\nTraining Neural Network on {handle_widget.value} tweets... This could take up to 3-5 minutes!\\n')\n",
        "                !python run_language_modeling.py \\\n",
        "                    --output_dir=output/$handle \\\n",
        "                    --overwrite_output_dir \\\n",
        "                    --model_type=gpt2 \\\n",
        "                    --model_name_or_path=gpt2 \\\n",
        "                    --do_train --train_data_file=$handle\\_train.txt \\\n",
        "                    --logging_steps 20 \\\n",
        "                    --per_gpu_train_batch_size 1 \\\n",
        "                    --num_train_epochs 4\n",
        "                \n",
        "                log_finetune.clear_output(wait=True)\n",
        "                print('\\nðŸŽ‰ Neural network trained successfully!')\n",
        "                \n",
        "                run_finetune.button_style = 'success'\n",
        "                run_predictions.disabled = False\n",
        "\n",
        "            except Exception as e:\n",
        "                print('An error occured...\\n')\n",
        "                print(e)\n",
        "                run_finetune.button_style = 'danger'\n",
        "        log_predictions.clear_output(wait=True)\n",
        "        with log_predictions:\n",
        "            print('\\nEnter the start of a sentence and click \"Run predictions\"')\n",
        "\n",
        "    run_finetune = widgets.Button(\n",
        "        description='Train Neural Network',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_finetune_clicked(b):\n",
        "        finetune()\n",
        "    run_finetune.on_click(on_run_finetune_clicked)\n",
        "\n",
        "    log_finetune = widgets.Output()\n",
        "    with log_finetune:\n",
        "        print('\\nFine-tune your model by clicking on \"Train Neural Network\"')\n",
        "\n",
        "    def clean_prediction(text):\n",
        "        token = '<|endoftext|>'\n",
        "        while len(token)>1:\n",
        "            text = text.replace(token, '')\n",
        "            token = token[:-1]\n",
        "        if text[-1] == '\"': text = text[:-1]\n",
        "        return text\n",
        "\n",
        "    predictions = []\n",
        "    \n",
        "    def predict():\n",
        "        handle = handle_widget.value[1:] if handle_widget.value[0] == '@' else handle_widget.value\n",
        "        start = start_widget.value\n",
        "        run_predictions.button_style = 'primary'\n",
        "        log_predictions.clear_output(wait=True)\n",
        "\n",
        "        def tweet_html(text):\n",
        "            max_char = 180\n",
        "            text = text[:max_char]\n",
        "            text = clean_prediction(text)\n",
        "            if len(text) > max_char: text = text[:max_char] + '...'\n",
        "\n",
        "            return '<a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" data-size=\"large\" '\\\n",
        "                    f'data-text=\"Trained a neural network on @{handle}: &quot;'\\\n",
        "                    f'{start}&quot; â†’ &quot;{text}&quot;\" '\\\n",
        "                    f'data-url=\"{wandb_url}\" data-hashtags=\"huggingtweets\" data-related=\"borisdayma,weights_biases,huggingface\"'\\\n",
        "                    'data-show-count=\"false\">Tweet</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>'\n",
        "        \n",
        "        try_success = False\n",
        "        with log_predictions:\n",
        "            try:\n",
        "                print(f'\\nPerforming predictions of {handle_widget.value} starting with \"{start}\"...\\nThis should take no more than 20 seconds!\\n')\n",
        "                \n",
        "                # load previous run\n",
        "                project = %env WANDB_PROJECT\n",
        "                wandb_id = wandb.api.list_runs(project)[0]['name']\n",
        "                wandb.init(id=wandb_id, resume='must')\n",
        "                wandb_url = wandb.run.get_url()\n",
        "                \n",
        "                seed = random.randint(0, 2**32-1)\n",
        "                val = !python run_generation.py \\\n",
        "                    --model_type gpt2 \\\n",
        "                    --model_name_or_path output/$handle \\\n",
        "                    --length 150 \\\n",
        "                    --stop_token \"{'\\n'}\" \\\n",
        "                    --num_return_sequences 5 \\\n",
        "                    --temperature 1 \\\n",
        "                    --seed $seed \\\n",
        "                    --prompt {'\"' + start + '\"'}\n",
        "                generated = [val[-1-2*k] for k in range(5)[::-1]]\n",
        "                generated = [clean_prediction(g) for g in generated]\n",
        "                log_predictions.clear_output(wait=True)\n",
        "                print(f'\\nPredictions of {handle_widget.value} starting with \"{start}\" via #huggingtweet')\n",
        "                for i, g in enumerate(generated):\n",
        "                    predictions.append([start, clean_prediction(g)])\n",
        "                \n",
        "                # log predictions\n",
        "                wandb.log({'examples': wandb.Table(data=predictions, columns=['Input', 'Prediction'])})\n",
        "                \n",
        "                # Update display name\n",
        "                wandb.run.name = handle\n",
        "                wandb.run.save()\n",
        "                \n",
        "                # display wandb run\n",
        "                print(f'\\nðŸš€ View all results under the \"Media\" panel at {click.style(wandb_url, underline=True, fg=\"blue\")}')\n",
        "                print('Try again new predictions or test other sentences!')\n",
        "\n",
        "                # make html table\n",
        "                tweet_data = [[tweet_html(g), g] for g in generated]\n",
        "                tweet_table = HTML(html_table(tweet_data))\n",
        "\n",
        "                run_predictions.button_style = 'success'\n",
        "                try_success = True\n",
        "                \n",
        "            except Exception as e:\n",
        "                print('An error occured...\\n')\n",
        "                print(e)\n",
        "                run_predictions.button_style = 'danger'\n",
        "\n",
        "        if try_success: display(tweet_table)\n",
        "                \n",
        "    start_widget = widgets.Text(value='My dream is',\n",
        "                                placeholder='Enter twitter handle')\n",
        "\n",
        "    run_predictions = widgets.Button(\n",
        "        description='Run predictions',\n",
        "        button_style='primary',\n",
        "        disabled=True)\n",
        "    def on_run_predictions_clicked(b):\n",
        "        predict()\n",
        "    run_predictions.on_click(on_run_predictions_clicked)\n",
        "\n",
        "    log_predictions = widgets.Output()\n",
        "    with log_predictions:\n",
        "        print('\\nWaiting for Step 2 to complete...')\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print('ðŸŽ‰ Environment set-up correctly!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMpSPr0T8AHD",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 - Download tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA4AFRTC8AHD",
        "colab_type": "text"
      },
      "source": [
        "We choose a Twitter user and download his tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "6O-8Kr_m8AHE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title â €\n",
        "display(widgets.VBox([handle_widget, run_dl_tweets, log_dl_tweets]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_ArgCZ8AHH",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 - Train your Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdTa0X4j8AHI",
        "colab_type": "text"
      },
      "source": [
        "We fine-tune a neural network on downloaded tweets using [Huggingface](https://huggingface.co/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "CpxBQYF88AHJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title â €\n",
        "display(widgets.VBox([run_finetune, log_finetune]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfRPh4V18AHM",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Visualize Predictions and Have Fun!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "facOJxhX8AHM",
        "colab_type": "text"
      },
      "source": [
        "Just start a sentence and let the model finish it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "FlMACi0-8AHN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title â €\n",
        "display(widgets.VBox([start_widget, run_predictions, log_predictions]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlRI2hsBKtz6",
        "colab_type": "text"
      },
      "source": [
        "Huggingtweets is still in its infancy and will get better over time!\n",
        "\n",
        "In the future, it will train continuously to become a Twitter expert!\n",
        "\n",
        "*Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4iawnVxrItM",
        "colab_type": "text"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L92nouh8AHU",
        "colab_type": "text"
      },
      "source": [
        "*Built by Boris Dayma ([@borisdayma](https://twitter.com/borisdayma))*\n",
        "\n",
        "My main goals with this project are:\n",
        "* to experiment with how to train, deploy and maintain neural networks in production ;\n",
        "* to make AI accessible to everyone.\n",
        "\n",
        "To see how the model works, visit the project repository.\n",
        "\n",
        "[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n",
        "\n",
        "<br>\n",
        "\n",
        "For additional resources, visit:\n",
        "\n",
        "* [A Step by Step Guide to Tracking Hugging Face Model Performance](https://app.wandb.ai/jxmorris12/huggingface-demo/reports/A-Step-by-Step-Guide-to-Tracking-Hugging-Face-Model-Performance--VmlldzoxMDE2MTU)\n",
        "* [W&B Forum](http://bit.ly/wandb-forum): If you have any questions, reach out to the slack community"
      ]
    }
  ]
}